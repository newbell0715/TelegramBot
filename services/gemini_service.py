import asyncio
import logging
import json
import os
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
import google.generativeai as genai
import pytz
from functools import lru_cache
from cachetools import TTLCache

from config.settings import GEMINI_API_KEY, MODEL_CONFIG

logger = logging.getLogger(__name__)

# ì‘ë‹µ ìºì‹œ (ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ë°˜ë³µ ìš”ì²­ ë°©ì§€)
response_cache = TTLCache(maxsize=500, ttl=1800)  # 30ë¶„ ìºì‹œ

class GeminiService:
    """í–¥ìƒëœ Gemini AI ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.model_status = self._load_model_status()
        self.current_model = None
        self._configure_api()
    
    def _load_model_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ë¡œë“œ"""
        try:
            if os.path.exists('model_status.json'):
                with open('model_status.json', 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception:
            pass
        
        return {
            'current_index': 0,
            'quota_exceeded_time': None,
            'last_primary_attempt': None,
            'failure_count': 0,
            'daily_requests': 0,
            'last_reset_date': datetime.now().date().isoformat()
        }
    
    def _save_model_status(self) -> None:
        """ëª¨ë¸ ìƒíƒœ ì €ì¥"""
        try:
            with open('model_status.json', 'w', encoding='utf-8') as f:
                json.dump(self.model_status, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ëª¨ë¸ ìƒíƒœ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _configure_api(self) -> None:
        """API ì„¤ì •"""
        genai.configure(api_key=GEMINI_API_KEY)
        self.current_model = self._get_model()
    
    def _get_model(self, idx: Optional[int] = None) -> genai.GenerativeModel:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        if idx is None:
            idx = self.model_status['current_index']
        
        model_config = MODEL_CONFIG[idx]
        return genai.GenerativeModel(
            model_name=model_config['name'],
            generation_config={
                'temperature': 0.7,
                'top_p': 0.8,
                'top_k': 40,
                'max_output_tokens': 2048,
            }
        )
    
    def _reset_daily_limits(self) -> None:
        """ì¼ì¼ ì œí•œ ë¦¬ì…‹"""
        today = datetime.now().date().isoformat()
        if self.model_status['last_reset_date'] != today:
            self.model_status['daily_requests'] = 0
            self.model_status['last_reset_date'] = today
            self._save_model_status()
    
    def _should_fallback_to_primary(self) -> bool:
        """ê¸°ë³¸ ëª¨ë¸ë¡œ ë³µê·€í• ì§€ í™•ì¸"""
        if self.model_status['current_index'] == 0:
            return False
        
        # í• ë‹¹ëŸ‰ ì´ˆê³¼ í›„ 24ì‹œê°„ ê²½ê³¼ ì‹œ ë³µê·€ ì‹œë„
        if self.model_status.get('quota_exceeded_time'):
            exceeded_time = datetime.fromisoformat(self.model_status['quota_exceeded_time'])
            if datetime.now() - exceeded_time > timedelta(hours=24):
                return True
        
        return False
    
    async def generate_content(self, prompt: str, use_cache: bool = True) -> str:
        """AI ì½˜í…ì¸  ìƒì„± (í–¥ìƒëœ ë²„ì „)"""
        # ìºì‹œ í™•ì¸
        if use_cache and prompt in response_cache:
            logger.info("ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©")
            return response_cache[prompt]
        
        self._reset_daily_limits()
        
        # ê¸°ë³¸ ëª¨ë¸ ë³µê·€ ì²´í¬
        if self._should_fallback_to_primary():
            self.model_status['current_index'] = 0
            self.model_status['failure_count'] = 0
            self._save_model_status()
        
        # ëª¨ë¸ë³„ë¡œ ì‹œë„
        for idx in range(self.model_status['current_index'], len(MODEL_CONFIG)):
            try:
                model = self._get_model(idx)
                
                # ë¹„ë™ê¸° ì‹¤í–‰
                response = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: model.generate_content(prompt)
                )
                
                if response and response.text:
                    # ì„±ê³µ ì‹œ í†µê³„ ì—…ë°ì´íŠ¸
                    self.model_status['daily_requests'] += 1
                    if idx != 0:
                        # í´ë°± ëª¨ë¸ ì„±ê³µ ì‹œ ë‹¤ìŒì— ê¸°ë³¸ ëª¨ë¸ ì‹œë„
                        self.model_status['current_index'] = 0
                        self.model_status['failure_count'] = 0
                    
                    self._save_model_status()
                    
                    # ìºì‹œ ì €ì¥
                    if use_cache:
                        response_cache[prompt] = response.text
                    
                    logger.info(f"âœ… {MODEL_CONFIG[idx]['display_name']} ì‚¬ìš© ì„±ê³µ")
                    return response.text
                
            except Exception as e:
                error_str = str(e).lower()
                logger.error(f"âŒ {MODEL_CONFIG[idx]['display_name']} ì—ëŸ¬: {e}")
                
                # í• ë‹¹ëŸ‰/429/404 ì—ëŸ¬ ì‹œ ë‹¤ìŒ ëª¨ë¸ë¡œ
                if any(keyword in error_str for keyword in 
                       ['quota', '429', 'rate limit', 'resource_exhausted', 'not found', '404']):
                    self.model_status['current_index'] = idx + 1
                    self.model_status['quota_exceeded_time'] = datetime.now().isoformat()
                    self.model_status['failure_count'] = 0
                    self._save_model_status()
                    continue
                
                # ê¸°íƒ€ ì—ëŸ¬ì˜ ê²½ìš° ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€
                self.model_status['failure_count'] += 1
                self._save_model_status()
                
                if self.model_status['failure_count'] >= 3 and idx < len(MODEL_CONFIG) - 1:
                    self.model_status['current_index'] = idx + 1
                    self.model_status['failure_count'] = 0
                    self._save_model_status()
                    continue
                
                # ì¬ì‹œë„ ë¡œì§
                if self.model_status['failure_count'] < 3:
                    await asyncio.sleep(2 ** self.model_status['failure_count'])  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    continue
        
        # ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨
        return self._get_fallback_response()
    
    def _get_fallback_response(self) -> str:
        """í´ë°± ì‘ë‹µ"""
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ˜…"
    
    async def translate_text(self, text: str, target_language: str, detailed: bool = False) -> str:
        """ë²ˆì—­ ì„œë¹„ìŠ¤"""
        if detailed:
            prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_language}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”: {text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ë²ˆì—­: [ìì—°ìŠ¤ëŸ¬ìš´ ë²ˆì—­]
2. ëŒ€ì•ˆ ë²ˆì—­: [ë‹¤ë¥¸ í‘œí˜„]
3. ë¬¸ë²• ì„¤ëª…: [ì£¼ìš” ë¬¸ë²• í¬ì¸íŠ¸]
4. ë°œìŒ ê°€ì´ë“œ: [ë°œìŒ ë„ì›€ë§]
"""
        else:
            prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_language}ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•´ì£¼ì„¸ìš”: {text}"
        
        return await self.generate_content(prompt)
    
    async def correct_writing(self, text: str, language: str = "ëŸ¬ì‹œì•„ì–´") -> str:
        """ì‘ë¬¸ êµì • ì„œë¹„ìŠ¤"""
        prompt = f"""
ë‹¹ì‹ ì€ ì¹œì ˆí•œ {language} ì›ì–´ë¯¼ ì„ ìƒë‹˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì¥ì„ êµì •í•´ì£¼ì„¸ìš”:

í•™ìƒ ë¬¸ì¥: "{text}"

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
ğŸ“ **ì›ë³¸**: {text}
âœ¨ **êµì •**: [ì˜¬ë°”ë¥¸ ë¬¸ì¥]
ğŸ“Š **ì ìˆ˜**: [1-10ì ]
ğŸ’¡ **ì„¤ëª…**: [êµ¬ì²´ì ì¸ êµì • ì´ìœ ]
ğŸ¯ **íŒ**: [í•™ìŠµì— ë„ì›€ë˜ëŠ” ì¡°ì–¸]
"""
        return await self.generate_content(prompt)
    
    async def generate_quiz_question(self, category: str, difficulty: str = "medium") -> str:
        """í€´ì¦ˆ ì§ˆë¬¸ ìƒì„±"""
        prompt = f"""
{difficulty} ë‚œì´ë„ì˜ ëŸ¬ì‹œì•„ì–´ {category} í€´ì¦ˆë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

í˜•ì‹:
â“ **ë¬¸ì œ**: [ì§ˆë¬¸]
A) [ì„ íƒì§€1]
B) [ì„ íƒì§€2] 
C) [ì„ íƒì§€3]
D) [ì„ íƒì§€4]

ì •ë‹µ: [ì •ë‹µ ì„¤ëª…]
"""
        return await self.generate_content(prompt)
    
    async def chat_response(self, message: str, context: Optional[str] = None) -> str:
        """ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ì‘ë‹µ"""
        if context:
            prompt = f"ì´ì „ ëŒ€í™” ë§¥ë½: {context}\n\nì‚¬ìš©ì ë©”ì‹œì§€: {message}\n\nì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì‘ë‹µì„ í•´ì£¼ì„¸ìš”."
        else:
            prompt = f"ì‚¬ìš©ì ë©”ì‹œì§€: {message}\n\nì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì‘ë‹µì„ í•´ì£¼ì„¸ìš”."
        
        return await self.generate_content(prompt, use_cache=False)
    
    def get_status(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´"""
        current_model = MODEL_CONFIG[self.model_status['current_index']]['display_name']
        
        return {
            'current_model': current_model,
            'daily_requests': self.model_status['daily_requests'],
            'is_primary': self.model_status['current_index'] == 0,
            'failure_count': self.model_status['failure_count'],
            'cache_size': len(response_cache),
            'last_reset': self.model_status['last_reset_date']
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiService()

# í¸ì˜ í•¨ìˆ˜ë“¤
async def call_gemini(prompt: str) -> str:
    """ê¸°ë³¸ Gemini í˜¸ì¶œ"""
    return await gemini_service.generate_content(prompt)

async def translate_with_gemini(text: str, target_language: str, detailed: bool = False) -> str:
    """ë²ˆì—­ í˜¸ì¶œ"""
    return await gemini_service.translate_text(text, target_language, detailed)

async def correct_with_gemini(text: str) -> str:
    """êµì • í˜¸ì¶œ"""
    return await gemini_service.correct_writing(text)

async def chat_with_gemini(message: str, context: Optional[str] = None) -> str:
    """ì±„íŒ… í˜¸ì¶œ"""
    return await gemini_service.chat_response(message, context) 